{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc909eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar pyspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "736e9537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Default.DESKTOP-GVJJTQ5\\anaconda3\\lib\\site-packages\\pyspark\\sql\\context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#inicializar SQLContext\n",
    "from pyspark import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d78cb3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar datos\n",
    "df = sqlContext.read.csv(\"heart.dat\", sep = \" \", inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3a6174b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: double (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      " |-- _c3: double (nullable = true)\n",
      " |-- _c4: double (nullable = true)\n",
      " |-- _c5: double (nullable = true)\n",
      " |-- _c6: double (nullable = true)\n",
      " |-- _c7: double (nullable = true)\n",
      " |-- _c8: double (nullable = true)\n",
      " |-- _c9: double (nullable = true)\n",
      " |-- _c10: double (nullable = true)\n",
      " |-- _c11: double (nullable = true)\n",
      " |-- _c12: double (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#revisar tipo de datos\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c08c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+-----+-----+---+---+-----+---+---+----+----+----+----+\n",
      "| _c0|_c1|_c2|  _c3|  _c4|_c5|_c6|  _c7|_c8|_c9|_c10|_c11|_c12|_c13|\n",
      "+----+---+---+-----+-----+---+---+-----+---+---+----+----+----+----+\n",
      "|70.0|1.0|4.0|130.0|322.0|0.0|2.0|109.0|0.0|2.4| 2.0| 3.0| 3.0|   2|\n",
      "|67.0|0.0|3.0|115.0|564.0|0.0|2.0|160.0|0.0|1.6| 2.0| 0.0| 7.0|   1|\n",
      "|57.0|1.0|2.0|124.0|261.0|0.0|0.0|141.0|0.0|0.3| 1.0| 0.0| 7.0|   2|\n",
      "|64.0|1.0|4.0|128.0|263.0|0.0|0.0|105.0|1.0|0.2| 2.0| 1.0| 7.0|   1|\n",
      "|74.0|0.0|2.0|120.0|269.0|0.0|2.0|121.0|1.0|0.2| 1.0| 1.0| 3.0|   1|\n",
      "+----+---+---+-----+-----+---+---+-----+---+---+----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Mostrar datos\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f228cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anos',\n",
       " 'sexo',\n",
       " 'tipo_dolor',\n",
       " 'presion_arterial_reposo',\n",
       " 'colesterol_serico',\n",
       " 'azucar',\n",
       " 'resl_electrocardiograficos',\n",
       " 'freq_cardiaca_max',\n",
       " 'angina',\n",
       " 'oldpeak',\n",
       " 'pendiente_st',\n",
       " 'vasos_fluoroscopia',\n",
       " 'thal',\n",
       " 'target']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Asignar nombres a variables\n",
    "columns = [\"anos\", \"sexo\", \"tipo_dolor\", \"presion_arterial_reposo\", \"colesterol_serico\", \"azucar\", \n",
    "              \"resl_electrocardiograficos\", \"freq_cardiaca_max\", \"angina\", \"oldpeak\", \"pendiente_st\", \"vasos_fluoroscopia\",\n",
    "             \"thal\", \"target\"]\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    df = df.withColumnRenamed('_c'+str(i), columns[i])\n",
    "    \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "113b27f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "093a5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agregar columna enfermo\n",
    "\n",
    "# from pyspark.sql.functions import col\n",
    "# x = df.select(col(\"thal\"))\n",
    "\n",
    "# #definir funciÃ³n\n",
    "# def enfermo (x):\n",
    "#     x.map(lambda x: 1 if x == \"7.0\" else 0)\n",
    "\n",
    "# df.transform(enfermo, x)\n",
    "\n",
    "from pyspark.sql.functions import  when\n",
    "df = df.withColumn('enfermo', when(df.thal>6, 1).otherwise(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "125d4515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- anos: double (nullable = true)\n",
      " |-- sexo: double (nullable = true)\n",
      " |-- tipo_dolor: double (nullable = true)\n",
      " |-- presion_arterial_reposo: double (nullable = true)\n",
      " |-- colesterol_serico: double (nullable = true)\n",
      " |-- azucar: double (nullable = true)\n",
      " |-- resl_electrocardiograficos: double (nullable = true)\n",
      " |-- freq_cardiaca_max: double (nullable = true)\n",
      " |-- angina: double (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- pendiente_st: double (nullable = true)\n",
      " |-- vasos_fluoroscopia: double (nullable = true)\n",
      " |-- thal: double (nullable = true)\n",
      " |-- enfermo: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#revisar tipo de datos\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "138aa940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertir enfermo a string\n",
    "#from pyspark.sql import types\n",
    "\n",
    "# CONTI_FEATURES  = [\"colesterol_serico\", \"freq_cardiaca_max\", \"oldpeak\", \"pendiente_st\"]\n",
    "# # # Convert the type\n",
    "# for i in CONTI_FEATURES:\n",
    "#     df = df.withColumn(i, df[i].cast(types.FloatType()))\n",
    "\n",
    "# INT_FEATURES = [\"anos\", \"vasos_fluoroscopia\", \"sexo \", \", \"azucar\"\"]\n",
    "# for i in INT_FEATURES:\n",
    "#     df = df.withColumn(i, df[i].cast(types.IntegerType()))\n",
    "\n",
    "#df = df.withColumn(\"enfermo\", df[\"enfermo\"].cast(types.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44b12fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- anos: double (nullable = true)\n",
      " |-- sexo: double (nullable = true)\n",
      " |-- tipo_dolor: double (nullable = true)\n",
      " |-- presion_arterial_reposo: double (nullable = true)\n",
      " |-- colesterol_serico: double (nullable = true)\n",
      " |-- azucar: double (nullable = true)\n",
      " |-- resl_electrocardiograficos: double (nullable = true)\n",
      " |-- freq_cardiaca_max: double (nullable = true)\n",
      " |-- angina: double (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- pendiente_st: double (nullable = true)\n",
      " |-- vasos_fluoroscopia: double (nullable = true)\n",
      " |-- thal: double (nullable = true)\n",
      " |-- enfermo: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#revisar tipo de datos\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b3880f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear variable feature\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Specify the input and output columns of the vector assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['anos',\n",
    "               'sexo',\n",
    "               'tipo_dolor',\n",
    "               'presion_arterial_reposo',\n",
    "               'colesterol_serico',\n",
    "               'azucar',\n",
    "               'resl_electrocardiograficos',\n",
    "               'freq_cardiaca_max',\n",
    "               'angina',\n",
    "               'oldpeak',\n",
    "               'pendiente_st',\n",
    "               'vasos_fluoroscopia'        \n",
    "    ],\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "# Transform the data\n",
    "df = assembler.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faa1f674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+-----------------------+-----------------+------+--------------------------+-----------------+------+-------+------------+------------------+----+-------+--------------------+\n",
      "|anos|sexo|tipo_dolor|presion_arterial_reposo|colesterol_serico|azucar|resl_electrocardiograficos|freq_cardiaca_max|angina|oldpeak|pendiente_st|vasos_fluoroscopia|thal|enfermo|            features|\n",
      "+----+----+----------+-----------------------+-----------------+------+--------------------------+-----------------+------+-------+------------+------------------+----+-------+--------------------+\n",
      "|70.0| 1.0|       4.0|                  130.0|            322.0|   0.0|                       2.0|            109.0|   0.0|    2.4|         2.0|               3.0| 3.0|      0|[70.0,1.0,4.0,130...|\n",
      "|67.0| 0.0|       3.0|                  115.0|            564.0|   0.0|                       2.0|            160.0|   0.0|    1.6|         2.0|               0.0| 7.0|      1|[67.0,0.0,3.0,115...|\n",
      "|57.0| 1.0|       2.0|                  124.0|            261.0|   0.0|                       0.0|            141.0|   0.0|    0.3|         1.0|               0.0| 7.0|      1|[57.0,1.0,2.0,124...|\n",
      "|64.0| 1.0|       4.0|                  128.0|            263.0|   0.0|                       0.0|            105.0|   1.0|    0.2|         2.0|               1.0| 7.0|      1|[64.0,1.0,4.0,128...|\n",
      "|74.0| 0.0|       2.0|                  120.0|            269.0|   0.0|                       2.0|            121.0|   1.0|    0.2|         1.0|               1.0| 3.0|      0|[74.0,0.0,2.0,120...|\n",
      "+----+----+----------+-----------------------+-----------------+------+--------------------------+-----------------+------+-------+------------+------------------+----+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0059d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# estandarizar\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(df)\n",
    "df = scalerModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a256f359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+-----------------------+-----------------+------+--------------------------+-----------------+------+-------+------------+------------------+----+-------+--------------------+--------------------+\n",
      "|anos|sexo|tipo_dolor|presion_arterial_reposo|colesterol_serico|azucar|resl_electrocardiograficos|freq_cardiaca_max|angina|oldpeak|pendiente_st|vasos_fluoroscopia|thal|enfermo|            features|      scaledFeatures|\n",
      "+----+----+----------+-----------------------+-----------------+------+--------------------------+-----------------+------+-------+------------+------------------+----+-------+--------------------+--------------------+\n",
      "|70.0| 1.0|       4.0|                  130.0|            322.0|   0.0|                       2.0|            109.0|   0.0|    2.4|         2.0|               3.0| 3.0|      0|[70.0,1.0,4.0,130...|[7.68465131046640...|\n",
      "|67.0| 0.0|       3.0|                  115.0|            564.0|   0.0|                       2.0|            160.0|   0.0|    1.6|         2.0|               0.0| 7.0|      1|[67.0,0.0,3.0,115...|[7.35530911144641...|\n",
      "|57.0| 1.0|       2.0|                  124.0|            261.0|   0.0|                       0.0|            141.0|   0.0|    0.3|         1.0|               0.0| 7.0|      1|[57.0,1.0,2.0,124...|[6.25750178137978...|\n",
      "|64.0| 1.0|       4.0|                  128.0|            263.0|   0.0|                       0.0|            105.0|   1.0|    0.2|         2.0|               1.0| 7.0|      1|[64.0,1.0,4.0,128...|[7.02596691242642...|\n",
      "|74.0| 0.0|       2.0|                  120.0|            269.0|   0.0|                       2.0|            121.0|   1.0|    0.2|         1.0|               1.0| 3.0|      0|[74.0,0.0,2.0,120...|[8.12377424249305...|\n",
      "+----+----+----------+-----------------------+-----------------+------+--------------------------+-----------------+------+-------+------------+------------------+----+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f706c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the 'features' and 'new_column' for the model\n",
    "model_data = df.select('features', 'enfermo')\n",
    "\n",
    "# Rename 'new_column' to 'label' as required by MLlib\n",
    "model_data = model_data.withColumnRenamed('enfermo', 'label')\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = model_data.randomSplit([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cb4da69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(12,[0,2,3,4,7,10...|    0|\n",
      "|(12,[0,2,3,4,7,10...|    0|\n",
      "|(12,[0,2,3,4,7,10...|    0|\n",
      "|(12,[0,2,3,4,7,10...|    0|\n",
      "|(12,[0,2,3,4,7,10...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92e47560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(12,[0,2,3,4,7,10...|    0|[3.14526709614831...|[0.95872182662437...|       0.0|\n",
      "|(12,[0,2,3,4,7,10...|    0|[2.50233666267183...|[0.92430546645575...|       0.0|\n",
      "|[34.0,1.0,1.0,118...|    0|[3.47057858151705...|[0.96983894741387...|       0.0|\n",
      "|[35.0,0.0,4.0,138...|    0|[2.19263853535136...|[0.89958649841182...|       0.0|\n",
      "|[35.0,1.0,4.0,120...|    1|[0.06724021769477...|[0.51680372375209...|       0.0|\n",
      "|[40.0,1.0,1.0,140...|    1|[2.36273187005977...|[0.91394091817381...|       0.0|\n",
      "|[40.0,1.0,4.0,110...|    1|[1.18160972373368...|[0.76523711292427...|       0.0|\n",
      "|[40.0,1.0,4.0,152...|    1|[0.19117629141765...|[0.54764903660952...|       0.0|\n",
      "|[41.0,1.0,2.0,110...|    0|[2.04802247115920...|[0.88574764787751...|       0.0|\n",
      "|[41.0,1.0,3.0,130...|    0|[0.59523805171585...|[0.64456609338453...|       0.0|\n",
      "|[42.0,0.0,4.0,102...|    0|[2.35388408402220...|[0.91324246048212...|       0.0|\n",
      "|[42.0,1.0,2.0,120...|    0|[1.35813892357835...|[0.79545705753963...|       0.0|\n",
      "|[42.0,1.0,3.0,130...|    0|[1.74490502410715...|[0.85130902358160...|       0.0|\n",
      "|[42.0,1.0,4.0,136...|    0|[-1.0144989936599...|[0.26610031400962...|       1.0|\n",
      "|[42.0,1.0,4.0,140...|    0|[0.30920008214768...|[0.57668999825944...|       0.0|\n",
      "|[43.0,0.0,3.0,122...|    0|[2.32043465321112...|[0.91055534701369...|       0.0|\n",
      "|[43.0,0.0,4.0,132...|    1|[1.14064616756639...|[0.75779825639222...|       0.0|\n",
      "|[43.0,1.0,4.0,120...|    1|[0.87585052086007...|[0.70596160972209...|       0.0|\n",
      "|[43.0,1.0,4.0,150...|    0|[-0.1779898578402...|[0.45561963931505...|       1.0|\n",
      "|[44.0,1.0,2.0,120...|    1|[1.54131437760377...|[0.82365571581629...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "# Create a Logistic Regression model and fit it to the training data\n",
    "lr = LogisticRegression()\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Show some predictions\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94d9c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccc2d49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.withColumn('correct', functions.expr('label == prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbc20479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+-------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|correct|\n",
      "+--------------------+-----+--------------------+--------------------+----------+-------+\n",
      "|(12,[0,2,3,4,7,10...|    0|[3.14526709614831...|[0.95872182662437...|       0.0|   true|\n",
      "|(12,[0,2,3,4,7,10...|    0|[2.50233666267183...|[0.92430546645575...|       0.0|   true|\n",
      "|[34.0,1.0,1.0,118...|    0|[3.47057858151705...|[0.96983894741387...|       0.0|   true|\n",
      "|[35.0,0.0,4.0,138...|    0|[2.19263853535136...|[0.89958649841182...|       0.0|   true|\n",
      "|[35.0,1.0,4.0,120...|    1|[0.06724021769477...|[0.51680372375209...|       0.0|  false|\n",
      "+--------------------+-----+--------------------+--------------------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
